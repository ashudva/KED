{
  
    
        "post0": {
            "title": "Credit Card Lead Prediction - EDA",
            "content": "Problem Statement . Credit Card Lead Prediction . Happy Customer Bank is a mid-sized private bank that deals in all kinds of banking products, like Savings accounts, Current accounts, investment products, credit products, among other offerings. The bank also cross-sells products to its existing customers and to do so they use different kinds of communication like tele-calling, e-mails, recommendations on net banking, mobile banking, etc. In this case, the Happy Customer Bank wants to cross sell its credit cards to its existing customers. The bank has identified a set of customers that are eligible for taking these credit cards. . Now, the bank is looking for your help to understand various patterns among the data that might be useful in identifying customers that could show higher intent towards a recommended credit card, given: . Customer details (gender, age, region etc.) | Details of his/her relationship with the bank (Channel_Code,Vintage, &#39;Avg_Asset_Value etc.) | Imports and Display Options . # Imports import warnings import numpy as np import pandas as pd import seaborn as sns from pathlib import Path import matplotlib.pyplot as plt # Set output display options warnings.filterwarnings(&#39;ignore&#39;) %matplotlib inline pd.set_option(&#39;display.max_columns&#39;, None) pd.set_option(&#39;display.max_rows&#39;, None) pd.options.display.float_format = &#39;{:.3f}&#39;.format # Set color palette for plots sns.set_palette(&#39;Set2&#39;) # Plots Background color bg_color = &#39;#f6f5f5&#39; . . Data Eyeballing . data_dir = Path(&#39;./data&#39;) df = pd.read_csv(data_dir / &#39;train.csv&#39;) df.drop(&#39;ID&#39;, axis=1, inplace=True) # Convert Is_Lead column into categorical variable df[&#39;Is_Lead&#39;] = df[&#39;Is_Lead&#39;].astype(&#39;category&#39;) df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 245725 entries, 0 to 245724 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 Gender 245725 non-null object 1 Age 245725 non-null int64 2 Region_Code 245725 non-null object 3 Occupation 245725 non-null object 4 Channel_Code 245725 non-null object 5 Vintage 245725 non-null int64 6 Credit_Product 216400 non-null object 7 Avg_Account_Balance 245725 non-null int64 8 Is_Active 245725 non-null object 9 Is_Lead 245725 non-null category dtypes: category(1), int64(3), object(6) memory usage: 17.1+ MB . Dataset can be considered small, as there are only 9 Independant variables and ~250K Entries. This information can be used to select the model and cross-validation strategy. . df.head() . Gender Age Region_Code Occupation Channel_Code Vintage Credit_Product Avg_Account_Balance Is_Active Is_Lead . 0 Female | 73 | RG268 | Other | X3 | 43 | No | 1045696 | No | 0 | . 1 Female | 30 | RG277 | Salaried | X1 | 32 | No | 581988 | No | 0 | . 2 Female | 56 | RG268 | Self_Employed | X3 | 26 | No | 1484315 | Yes | 0 | . 3 Male | 34 | RG270 | Salaried | X1 | 19 | No | 470454 | No | 0 | . 4 Female | 30 | RG282 | Salaried | X1 | 33 | No | 886787 | No | 0 | . # Categorical cols cat_cols = df.select_dtypes(include=[&#39;category&#39;, &#39;object&#39;]).columns # Numerical cols num_cols = df.select_dtypes(include=[&#39;int64&#39;]).columns print(f&#39;&#39;&#39; {len(cat_cols)}-Categorical Columns: {cat_cols.tolist()}, {len(num_cols)}-Numerical Columns: {num_cols.tolist()} &#39;&#39;&#39;) . . 7-Categorical Columns: [&#39;Gender&#39;, &#39;Region_Code&#39;, &#39;Occupation&#39;, &#39;Channel_Code&#39;, &#39;Credit_Product&#39;, &#39;Is_Active&#39;, &#39;Is_Lead&#39;], 3-Numerical Columns: [&#39;Age&#39;, &#39;Vintage&#39;, &#39;Avg_Account_Balance&#39;] . df.isnull().sum() . Gender 0 Age 0 Region_Code 0 Occupation 0 Channel_Code 0 Vintage 0 Credit_Product 29325 Avg_Account_Balance 0 Is_Active 0 Is_Lead 0 dtype: int64 . print(f&#39;{df.isnull().sum().sum() / df.shape[0] * 100: .3f}% values in &#39;Credit_Product &#39; are missing&#39;) . 11.934% values in &#39;Credit_Product&#39; are missing . print(df[&#39;Vintage&#39;].nunique(), f&#39;Unique values in &#39;Vintage &#39;&#39;, df[&#39;Age&#39;].nunique(), f&#39;Unique values in &#39;Age &#39;&#39;) . 66 Unique values in &#39;Vintage&#39; 63 Unique values in &#39;Age&#39; . Descriptive Statistics . df.describe() . Age Vintage Avg_Account_Balance . count 245725.000 | 245725.000 | 245725.000 | . mean 43.856 | 46.959 | 1128403.101 | . std 14.829 | 32.353 | 852936.356 | . min 23.000 | 7.000 | 20790.000 | . 25% 30.000 | 20.000 | 604310.000 | . 50% 43.000 | 32.000 | 894601.000 | . 75% 54.000 | 73.000 | 1366666.000 | . max 85.000 | 135.000 | 10352009.000 | . df.describe(include=[&#39;O&#39;, &#39;category&#39;]) . Gender Region_Code Occupation Channel_Code Credit_Product Is_Active Is_Lead . count 245725 | 245725 | 245725 | 245725 | 216400 | 245725 | 245725 | . unique 2 | 35 | 4 | 4 | 2 | 2 | 2 | . top Male | RG268 | Self_Employed | X1 | No | No | 0 | . freq 134197 | 35934 | 100886 | 103718 | 144357 | 150290 | 187437 | . Univariate Analysis . fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 9)) for i, col in enumerate(num_cols): sns.kdeplot(df[col], shade=True, label=col, ax=axes[0, i], color=&#39;Tomato&#39;) sns.distplot(df[col], hist=True, kde=False, label=col, bins=20, ax=axes[1, i]) sns.boxplot(x=col, data=df, ax=axes[2, i], color=&#39;#D1E4CD&#39;) # set title and remove x-axis labels axes[0, i].set_xlabel(&quot;&quot;) axes[1, i].set_xlabel(&quot;&quot;) axes[2, i].set_xlabel(&quot;&quot;) axes[0, i].set_ylabel(&quot;&quot;) axes[0, i].set_title(col) # Remove Spines for spine in [&#39;right&#39;, &#39;top&#39;]: axes[0, i].spines[spine].set_visible(False) axes[1, i].spines[spine].set_visible(False) axes[2, i].spines[spine].set_visible(False) plt.tight_layout() . We can use log-transform to make the distribution of &#39;Avg_Account_Balance&#39; more normal, as it approximately follows a log-normal distribution. . log_aab = np.log(df[&#39;Avg_Account_Balance&#39;]) fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 3), dpi=100) fig.suptitle(&#39;Distribution of log(Avg_Account_Balance)&#39;) sns.distplot(log_aab, label=&#39;log(Avg_Account_Balance)&#39;, kde=True, hist=True, bins=20, color=&#39;#19A789&#39;, ax=axes[0], kde_kws={&#39;color&#39;: &#39;Tomato&#39;}) sns.boxplot(x=log_aab, ax=axes[1], color=&#39;#69A789&#39;) axes[0].set_xlabel(&quot;&quot;) axes[1].set_xlabel(&quot;&quot;) # Remove Spines for spine in [&#39;right&#39;, &#39;top&#39;]: axes[0].spines[spine].set_visible(False) axes[1].spines[spine].set_visible(False) plt.show() . fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 2), dpi=300) for i, col in enumerate(num_cols): sns.violinplot(x=col, data=df, ax=axes[i]) axes[i].set_xlabel(&quot;&quot;) axes[i].set_ylabel(&quot;&quot;) axes[i].set_title(&quot; &quot;.join(col.split(&#39;_&#39;)), weight=&#39;bold&#39;) # Remove Spines for spine in [&#39;left&#39;, &#39;right&#39;, &#39;top&#39;]: axes[i].spines[spine].set_visible(False) axes[i].axes.get_yaxis().set_visible(False) . corr = df[num_cols].corr() fig, ax = plt.subplots(figsize=(6, 6)) sns.heatmap(corr, annot=True, fmt=&#39;.2f&#39;, ax=ax) plt.show() . There is a positive correlation between Vintage and Age which can be further explored using boxplot and scatterplot. No other numerical variables have a strong correlation. . # plot() for all categorical columns cat_feats = list(cat_cols) cat_feats.remove(&#39;Region_Code&#39;) plt.rcParams[&#39;figure.dpi&#39;] = 600 fig = plt.figure(figsize=(15, 9), facecolor=&#39;#f6f5f5&#39;) gs = fig.add_gridspec(2, 3) gs.update(wspace=0.25, hspace=.25) for i, col in enumerate(cat_feats): ax = fig.add_subplot(gs[i // 3, i % 3]) # Get the sorted value_counts and plot the bar plot col_data = df[col].value_counts(normalize=True) .rename(&#39;Percentage&#39;).mul(100) .reset_index().sort_values(ascending=False, by=&#39;Percentage&#39;) # Plot customizations for spine in [&#39;right&#39;, &#39;top&#39;, &#39;left&#39;]: ax.spines[spine].set_visible(False) ax.set_facecolor(bg_color) ax.axes.yaxis.set_visible(False) ax_sns = sns.barplot(x=col_data[&#39;index&#39;], y=col_data[&#39;Percentage&#39;], ax=ax, saturation=1) # Customize plot if i % 3 == 0: ax_sns.set_ylabel(&quot;Count(%)&quot;, weight=&#39;bold&#39;) else: ax_sns.set_ylabel(&quot;&quot;) ax_sns.set_xlabel(&quot; &quot;.join(col.split(&#39;_&#39;)), weight=&#39;bold&#39;) # Add patches for data percentages for p in ax.patches: value = f&#39;{p.get_height(): .0f}%&#39; x = p.get_x() + p.get_width() / 2 - 0.1 y = p.get_y() + p.get_height() + 2 ax.text(x, y, value, ha=&#39;left&#39;, va=&#39;center&#39;, fontsize=7, bbox=dict(facecolor=&#39;none&#39;, edgecolor=&#39;black&#39;, boxstyle=&#39;round&#39;, linewidth=0.5)) . . Dataset is highly imbalanced, as there are only 24% entries which are leads and 76% entries which are not. Due to the small size of the dataset, Upsampling Strategy to hadle imbalanced data could work well. . There is also a large imbalance in the dataset for Occupation and Channel_Code categories as there are very few entries for Enterpreneur and X4 respectively. . plt.figure(figsize=(16, 5), dpi = 600) col_data = df[&#39;Region_Code&#39;].value_counts(normalize=True) .rename(&#39;Percentage&#39;).mul(100).reset_index().sort_values(ascending=False, by=&#39;Percentage&#39;) sns.barplot(x=col_data[&#39;index&#39;], y=col_data[&#39;Percentage&#39;], saturation=1) plt.xticks(rotation=90) plt.xlabel(&#39;Region Code&#39;, weight=&#39;bold&#39;) plt.ylabel(&#39;Count(%)&#39;, weight=&#39;bold&#39;) plt.title(&#39;Ranked Frequency Plot - Region Code&#39;, weight=&#39;bold&#39;, fontsize=15) for spine in [&#39;top&#39;, &#39;right&#39;]: plt.gca().spines[spine].set_visible(False) plt.show() . Bivariate Analysis . fig = plt.figure(facecolor=bg_color, dpi = 600) g = sns.FacetGrid(df, col=&#39;Region_Code&#39;, col_wrap=6) g.map(sns.countplot, &#39;Is_Lead&#39;, saturation=1) plt.show() . &lt;Figure size 3600x2400 with 0 Axes&gt; . fig, axes = plt.subplots(nrows=len(cat_feats), ncols=3, figsize=(12, len(cat_cols)*2), dpi=600) for i, cat_col in enumerate(cat_feats): for j, num_col in enumerate(num_cols): ax_sns = sns.kdeplot(x = num_col, data=df, hue=cat_col, label=col, ax=axes[i, j]) # Customize the plot ax_sns.tick_params(axis=&#39;y&#39;, labelsize=0) axes[i, j].set_xlabel(&quot; &quot;.join(num_col.split(&#39;_&#39;)), weight=&#39;bold&#39;) if j != 0: axes[i, j].set_ylabel(&quot;&quot;) axes[i, j].legend_.remove() else: axes[i, j].set_ylabel(&quot;Density&quot;, weight=&#39;bold&#39;) for spine in [&#39;top&#39;, &#39;right&#39;]: axes[i, j].spines[spine].set_visible(False) plt.tight_layout() . Almost all the categories in each categorical variable follows the same distribution. . fig, axes = plt.subplots(nrows=len(cat_feats), ncols=3, figsize=(12, 3*len(cat_feats)), dpi=600) for i, cat_col in enumerate(cat_feats): for j, num_col in enumerate(num_cols): ax_sns = sns.boxplot(y=num_col, x=cat_col, data=df, ax=axes[i,j]) # Customize the plot axes[i, j].set_ylabel(num_col, weight=&#39;bold&#39;) axes[i, j].set_xlabel(cat_col, weight=&#39;bold&#39;) for spine in [&#39;top&#39;, &#39;right&#39;]: axes[i, j].spines[spine].set_visible(False) plt.tight_layout() . There are huge number of outliers for each categorical variable in Avg_Account_Balance. . grid = sns.FacetGrid(df, row=&#39;Occupation&#39;, col=&#39;Is_Active&#39;, hue=&#39;Is_Lead&#39;, aspect=1.5, size=4, palette=&#39;Set2&#39;) grid.map(plt.scatter, &#39;Age&#39;, &#39;Avg_Account_Balance&#39;, alpha=0.5) grid.add_legend() plt.show() . grid = sns.FacetGrid(df, row=&#39;Occupation&#39;, col=&#39;Credit_Product&#39;, hue=&#39;Is_Lead&#39;, aspect=1.5, size=4, palette=&#39;Set2&#39;) grid.map(plt.scatter, &#39;Age&#39;, &#39;Avg_Account_Balance&#39;, alpha=0.5) grid.add_legend() plt.show() . grid = sns.FacetGrid(df, row=&#39;Occupation&#39;, col=&#39;Gender&#39;, hue=&#39;Is_Lead&#39;, aspect=1.5, size=4, palette=&#39;Set2&#39;) grid.map(plt.scatter, &#39;Age&#39;, &#39;Avg_Account_Balance&#39;, alpha=0.5) grid.add_legend() plt.show() . grid = sns.FacetGrid(df, row=&#39;Occupation&#39;, col=&#39;Channel_Code&#39;, hue=&#39;Is_Lead&#39;, aspect=1.5, size=4, palette=&#39;Set2&#39;) grid.map(plt.scatter, &#39;Age&#39;, &#39;Avg_Account_Balance&#39;, alpha=0.5) grid.add_legend() plt.show() . From above all Scatter-Plots, we can see that almost every Self_Employed individual whose Age is above 40 is a lead. We can use this information to create a new feature. . Relationship between Missing Values and Target . # Fraction of customers with missing values who are leads df[&#39;Credit_Product&#39;][df[&#39;Credit_Product&#39;].isnull()] = &#39;Null&#39; na_df = df[&#39;Credit_Product&#39;].value_counts() .rename(&#39;Fraction&#39;).reset_index().sort_values(ascending=False, by=&#39;Fraction&#39;) fig = plt.figure(dpi = 600, figsize=(13,3)) ax_sns = sns.barplot(y=na_df[&#39;index&#39;], x=na_df[&#39;Fraction&#39;], orient=&#39;h&#39;, saturation=1, palette=&#39;flare&#39;) # Remove Spines for spine in [&#39;top&#39;, &#39;right&#39;, &#39;bottom&#39;]: plt.gca().spines[spine].set_visible(False) # Disable ticks plt.tick_params(axis=&#39;x&#39;, which=&#39;both&#39;, bottom=False, top=False, labelbottom=False) # Customize the plot plt.xlabel(&#39;Fraction of Customers&#39;, weight=&#39;bold&#39;, fontsize=13) plt.ylabel(&#39;Credit Product&#39;, weight=&#39;bold&#39;, fontsize=13) # Add patches for p in ax_sns.patches: value = f&#39;{p.get_width(): .0f} | {p.get_width() / df.shape[0] * 100: 0.2f}%&#39; x = p.get_x() + p.get_width() + 5e3 y = p.get_y() + p.get_height() / 2 ax_sns.text(x, y, value, ha=&#39;left&#39;, va=&#39;center&#39;, fontsize=9, bbox=dict(facecolor=&#39;none&#39;, edgecolor=&#39;black&#39;, boxstyle=&#39;round&#39;, linewidth=0.5)) plt.show() . . na_df = df.groupby(&#39;Credit_Product&#39;)[&#39;Is_Lead&#39;].value_counts(normalize=True) # convert na_df to dataframe na_df = na_df.reset_index() na_df.columns = [&#39;Credit_Product&#39;, &#39;Is_Lead&#39;, &#39;Fraction&#39;] # swap rows at index 2 and 3 na_df.loc[2], na_df.loc[3] = na_df.loc[3], na_df.loc[2] na_df . Credit_Product Is_Lead Fraction . 0 No | 0 | 0.926 | . 1 No | 1 | 0.074 | . 2 Null | 0 | 0.148 | . 3 Null | 1 | 0.852 | . 4 Yes | 0 | 0.685 | . 5 Yes | 1 | 0.315 | . fig = plt.figure(dpi=600, figsize=(13, 3)) ax_sns = sns.countplot(y=&#39;Credit_Product&#39;, data=df, hue=&#39;Is_Lead&#39;, palette=&#39;flare&#39;, saturation=1, orient=&#39;h&#39;) ax_sns.set_xlabel(&#39;Customers&#39;, weight=&#39;bold&#39;, fontsize=13) ax_sns.set_ylabel(&#39;Credit Product&#39;, weight=&#39;bold&#39;, fontsize=13) # Remove ticks plt.gca().tick_params(axis=&#39;x&#39;, which=&#39;both&#39;, bottom=False, labelbottom=False) # Remove Spines for spine in [&#39;top&#39;, &#39;right&#39;, &#39;bottom&#39;]: ax_sns.spines[spine].set_visible(False) # Add patches na_f = na_df[&#39;Fraction&#39;].values idx = 0 for p in ax_sns.patches: if idx &lt;= na_df.shape[0] // 2: value = f&quot;{na_f[idx] * 100: 0.2f}%&quot; idx += 2 elif idx == na_df.shape[0] // 2 + 1: value = f&quot;{na_f[idx] * 100: 0.2f}%&quot; idx = 1 else: value = f&quot;{na_f[idx] * 100: 0.2f}%&quot; idx += 2 x = p.get_x() + p.get_width() + 3e3 y = p.get_y() + p.get_height() / 2 ax_sns.text(x, y, value, ha=&#39;left&#39;, va=&#39;center&#39;, fontsize=8, bbox=dict(facecolor=&#39;none&#39;, edgecolor=&#39;black&#39;, boxstyle=&#39;round&#39;, linewidth=0.5)) plt.show() . . Calculate Feature Importance using Mutual Information . from sklearn.feature_selection import mutual_info_classif from sklearn.preprocessing import OrdinalEncoder, RobustScaler cat_cols = cat_cols.tolist() cat_cols.remove(&#39;Is_Lead&#39;) # Scale the numerical columns scaler = RobustScaler() df[num_cols] = scaler.fit_transform(df[num_cols]) # Encode categorical columns encoder = OrdinalEncoder() df[cat_cols] = encoder.fit_transform(df[cat_cols]) # Get the target variable target = df.pop(&#39;Is_Lead&#39;) # Get the indices of the categorical features cols = df.columns.tolist() cat_idx = [i for i in range(len(cols)) if cols[i] in cat_cols] # Calculate Mutual Information mi = mutual_info_classif(df, target, discrete_features=cat_idx) mi_df = pd.DataFrame(mi, index=df.columns, columns=[&#39;MI&#39;]) mi_df = mi_df.sort_values(by=&#39;MI&#39;, ascending=False) mi_df.head() . . MI . Credit_Product 0.161 | . Age 0.051 | . Channel_Code 0.048 | . Vintage 0.047 | . Occupation 0.011 | . fig = plt.figure(dpi=600, figsize=(6, 2)) sns.barplot(x=&#39;MI&#39;, y=mi_df.index, data=mi_df, palette=&#39;crest&#39;, saturation=1) # Remove ticks # plt.gca().tick_params(axis=&#39;x&#39;, which=&#39;both&#39;, bottom=False, labelbottom=False) # Remove Spines for spine in [&#39;top&#39;, &#39;right&#39;]: plt.gca().spines[spine].set_visible(False) # Remove &quot;_&quot; from yticklabels plt.gca().set_yticklabels(mi_df.index.str.replace(&#39;_&#39;, &#39; &#39;), fontsize=5, weight=&#39;bold&#39;) plt.gca().set_xticklabels(plt.gca().get_xticks(), fontsize=5) plt.xlabel(&#39;Mutual Information&#39;, fontsize=7) plt.show() .",
            "url": "https://www.ashishdev.codes/project/machine-learning/notebook/python/2021/11/30/EDA.html",
            "relUrl": "/project/machine-learning/notebook/python/2021/11/30/EDA.html",
            "date": " • Nov 30, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Solution Approach - Analytics Vidhya November Jobathoon",
            "content": "Table of Contents . Problem Statement | Data Wrangling | Results from the EDA | Feature Engineering | Modeling | . Problem Statement . Develop a Machine Learning model to aid HR Department in predicting the attrition of employees. . Given . Train Dataset - Given with the following features: . Demographics of the employee | Tenure information | Historical data regarding the performance | Target - There is no apparent target variable given in the dataset. The target is given in the column LastWorkingDate of the dataset. Target={DateIf  the employee did not leave the companyNullIf  the employee left the companyTarget = begin{cases} Date &amp; text{If } the employee did not leave the company Null &amp; text{If } the employee left the company end{cases}Target={DateNull​If  the employee did not leave the companyIf  the employee left the company​ . Filled the missing values in LastWorkingDate column with the ReportingDate column. . Test Dataset - Given only the Employee ID’s for which we need to predict if they will leave the company or not in the next two quarters of year 2018. . Evaluation Metric - F1 Score . Data Wrangling . To make the data more suitable for Machine Learning models, EDA, and Feature Engineering, a few Data Wrangling steps were taken: . Creating the Target Variable from the LastWorkingDate column | Fill missing values in the LastWorkingDate column with the ReportingDate column | Convert all the Date columns to datetime format | Results from the EDA . Following are the most prominent observations made from the EDA: . KDE Plots showed that the Age and Salary were normally distributed. | Total Business Value had a lot of zero values. | Salary and Age have a similar distribution for each Gender, City Category. | Employees with less Quarterly Rating tend to leave the company. | Older Employees have a much less probability of leaving the company, So JoiningYear, Tenure (in days, months, and years) would be great features to use in the model. | Employees who did not leave the company have a much higher number of Positive Total Business Value. | Feature Engineering . Features that were created based on EDA: . JoiningYear - Year in which the employee joined the company. | WorkingDays - Number of working days till the reporting date. | WorkingMonths - Number of working months till the reporting date. | WorkingYears - Number of working years till the reporting date. | Promotions - Number of promotions till the reporting date. | Quarterly_Rating_RA - Running Average of the QuarterlyRating. | Quarterly_Rating_CumSum - Cumulative sum of the QuarterlyRating. | Total_Business_Value_CumSum - Cumulative sum of the Total Business Value. | PBVCount - Number of positive Total Business Value till the reporting date. | NBVCount - Number of negative Total Business Value till the reporting date. | SalaryGrowth - Growth in Salary till the reporting date. | SalaryGrowthRatio - Growth in Salary till the reporting date as a ratio. | SalaryGrowth_WorkingDays - Ratio of SalaryGrowth to WorkingDays | SalaryGrowth_WorkingMonths - Ratio of SalaryGrowth to WorkingMonths | SalaryGrowth_WorkingYears - Ratio of SalaryGrowth to WorkingYears | Designation_Count - Total number of employees with the same designation till the reporting date. | ReportingDate_Count - Total number of employees reported on a reporting date. | City_ReprotingDate_Count - Total number of employees reported in a particular city on a reporting date. | City_Count - Total number of employees in a particular city. | Steps to assess the features: . Calculate the Mutual Information between the features and the target variable. | Create a Bar Plot to show the Mutual Information. | Get 5-Fold Cross Validation Score for a CatBoost Base model (without Hyper Parameter Tuning). | Plot the Feature Importance of the CatBoost model. | Discarded Features based on assessment: . SalaryGrowth_WorkingDays | SalaryGrowth_WorkingMonths | Designation_Count | SalaryGrowthRatio | SalaryGrowth | SalaryGrowth_WorkingYears | NBVCount | City_Count | Modeling . List of models used during the model-building: . XGBoost | CatBoost | LightGBM | Random Forest | MLP (with EaryStopping and ReduceLROnPlateau) | Results from the model building: . Used 5-Fold StratifiedKFold cross validation to assess the model at every step. | MLP did not perform well due to less data available for training, and quickly overfit. | XGBoost, LightGBM, and Radom Forest performed worse than the CatBoost model. | Hyper Parameter Tuning was done for all the models using Optuna. | All models were trained using Early Stopping Rounds. | Models other than CatBoost did not improve the score even after Hyper Parameter Tuning. | CatBoost model was the best performing model. | Final Model . Ensemble of 14 CatBoost Models with different Hyper Parameters and Random SEEDs. | Average of 5-Fold out-of-fold scored predictions from each model to create Meta Features. | Trained a Logistic Regression model on the meta features. | Final Inference on the Test Dataset - Using the Logistic Regression model. |",
            "url": "https://www.ashishdev.codes/markdown/jobathon/competetion/analytics/2021/11/22/Approach_AV_Jobathon_Nov2021.html",
            "relUrl": "/markdown/jobathon/competetion/analytics/2021/11/22/Approach_AV_Jobathon_Nov2021.html",
            "date": " • Nov 22, 2021"
        }
        
    
  
    
  
    
  
    
        ,"post4": {
            "title": "Authorship Identification: Part-1 (The baseline)",
            "content": "Abstract . Authorship identification is the task of identifying the author of a given text from a set of suspects. The main concern of this task is to define an appropriate characterization of texts that captures the writing style of authors. As a published author usually has a unique writing style in his/her work. The writing style is mostly context independent and is discernible by a human reader. In previous studies various stylometric models have been suggested for the aforementioned task e.g. BiLSTM, SVM, Logistic Regression, several other Deep Learning Models. But most of them fail or show poor results for either short passages or long passages and none of them were able to perform well in both cases. Previously the best performance at authroship identification is achieved by LSTM and GRU model. . Baseline Model . For setting up a baseline for the task, I used a combination of stack of 1D-CNN with BiLSTM which gives a validation accuracy: ~62% and test accuracy: ~54% while using a fairly simple BiDirectional LSTM and CNN Architecture. And unsurprisingly the results of baseline model are pretty close to the past best performing model without any type of Tuning. . Model uses pretrained GloVe word Embeddings for text representation. GloVe uncased word embeddings were trained using Wikipedia 2014 + Gigaword 5 and it consists of 6B tokens, 400K vocab. Embeddings are available as 50d, 100d, 200d, &amp; 300d vectors. [Source] . In most of the cases training word embeddings for the downstream task is a good idea and gives better results, albeit because of computational requirements I have used pretrained GloVE embeddings. . Utilitiy functions . These are the functions that I&#39;ll be using to do redundant tasks in this part like: . Plotting train history | Saving figures | Saving and Loading pickle objects | take a look if interested! . import matplotlib.pyplot as plt import numpy as np import pickle from pathlib import Path import keras def save_object(obj: object, file_path: Path) -&gt; None: &quot;&quot;&quot; Save a python object to the disk and creates the file if does not exists already. Args: file_path - Path object for pkl file location obj - object to be saved Returns: None &quot;&quot;&quot; if not file_path.exists(): file_path.touch() print(f&quot;pickle file {file_path.name} created successfully!&quot;) else: print(f&quot;pickle file {file_path.name} already exists!&quot;) with file_path.open(mode=&#39;wb&#39;) as file: pickle.dump(obj, file, protocol=pickle.HIGHEST_PROTOCOL) print(f&quot;object {type(obj)} saved to file {file_path.name}!&quot;) def load_object(file_path: Path) -&gt; object: &quot;&quot;&quot; Loads the pickle object file from the disk. Args: file_path - Path object for pkl file location Returns: object &quot;&quot;&quot; if file_path.exists(): with file_path.open(mode=&#39;rb&#39;) as file: print(f&quot;loaded object from file {file_path.name}&quot;) return pickle.load(file) else: raise FileNotFoundError def vectorize_sequence(sequences: np.ndarray, dimension: int = 10000): &quot;&quot;&quot; Convert sequences into one-hot encoded matrix of dimension [len(sequence), dimension] Args: sequences - ndarray of shape [samples, words] dimension = number of total words in vocab Return: vectorized sequence of shape [samples, one-hot-vecotor] &quot;&quot;&quot; # Create all-zero matrix results = np.zeros((len(sequences), dimension)) for (i, sequence) in enumerate(sequences): results[i, sequence] = 1. return results def plot_history( history: keras.callbacks.History, metric: str = &#39;acc&#39;, save_path: Path = None, model_name: str = None ) -&gt; None: &quot;&quot;&quot; Plots the history of training of a model during epochs Args: history: model history - training history of a model metric - Plots: 1. Training and Validation Loss 2. Training and Validation Accuracy &quot;&quot;&quot; f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5)) ax1.plot(history.epoch, history.history.get( &#39;loss&#39;), &quot;o&quot;, label=&#39;train loss&#39;) ax1.plot(history.epoch, history.history.get( &#39;val_loss&#39;), &#39;-&#39;, label=&#39;val loss&#39;) ax2.plot(history.epoch, history.history.get( metric), &#39;o&#39;, label=&#39;train acc&#39;) ax2.plot(history.epoch, history.history.get( f&quot;val_{metric}&quot;), &#39;-&#39;, label=&#39;val acc&#39;) ax1.set_xlabel(&quot;epoch&quot;) ax1.set_ylabel(&quot;loss&quot;) ax2.set_xlabel(&quot;epoch&quot;) ax2.set_ylabel(&quot;accuracy&quot;) ax1.set_title(&quot;Loss&quot;) ax2.set_title(&quot;Accuracy&quot;) f.suptitle(f&quot;Training History: {model_name}&quot;) ax1.legend() ax2.legend() if save_path is not None: f.savefig(save_path) . . Structure of notebook . Data Preprocessing a. Load dataset b. Text Vectorization c. Configure Dataset for faster training | Modelling a. Parse Glove Embeddings b. Define ConvnetBiLSTM model c. Load Embedding matrix | Training and Evaluation | Data Preprocessing . Dataset: UCI C50 Dataset(small subset of origin RCV1 dataset) [Source] C50 dataset is widely used for authorship identification. Dataset Specifications: . Catagories/Authors:50 &gt; Datapoints per class:50 &gt; Total Datapoints:5000 (4500 train, 500 test) . Loading and Preprocessing the dataset . 80-20 train and validation split and 500 holdout datapoints. | I&#39;ll use text_dataset_from_directory utility of keras library to load dataset which is faster than manually reading the text. | In the preprocessing step, numbers and special characters except {.} {,} and {&#39;} are removed from the dataset. | # Import Python Regular Expression library import re src_dir = Path(&#39;data/C50_raw/&#39;) src_test_dir = src_dir / &#39;test&#39; src_train_dir = src_dir / &#39;train&#39; dst_dir = Path(&#39;data/C50/&#39;) dst_test_dir = dst_dir / &#39;test&#39; dst_train_dir = dst_dir / &#39;train&#39; test_sub_dirs = src_test_dir.iterdir() train_sub_dirs = src_train_dir.iterdir() for i, author in enumerate(test_sub_dirs): author_name = author.name dst_author = dst_test_dir / author_name dst_author.mkdir() for file in author.iterdir(): file_name = file.name dst = dst_author / file_name raw_text = file.read_text(encoding=&#39;utf-8&#39;) out_text = re.sub(&quot;[^A-Za-z.&#39;,]+&quot;, &quot; &quot;, raw_text) dst.write_text(out_text, encoding=&#39;utf-8&#39;) for i, author in enumerate(train_sub_dirs): author_name = author.name dst_author = dst_train_dir / author_name dst_author.mkdir() for file in author.iterdir(): file_name = file.name dst = dst_author / file_name raw_text = file.read_text(encoding=&#39;utf-8&#39;) out_text = re.sub(&quot;[^A-Za-z.&#39;,]+&quot;, &quot; &quot;, raw_text) dst.write_text(out_text, encoding=&#39;utf-8&#39;) . . nfiles_test = len(list(dst_test_dir.glob(&quot;*/*.txt&quot;))) nfiles_train = len(list(dst_train_dir.glob(&quot;*/*.txt&quot;))) print(f&quot;Number of files in processed test dataset: {nfiles_test}&quot;) print(f&quot;Number of files in processed train dataset: {nfiles_train}&quot;) . . Number of files in processed test dataset: 500 Number of files in processed train dataset: 4500 . Load Dataset . import keras import numpy as np import tensorflow as tf from keras import models, layers from keras.preprocessing import text_dataset_from_directory from keras.layers.experimental.preprocessing import TextVectorization import keras.callbacks as cb ds_dir = Path(&#39;data/C50/&#39;) train_dir = ds_dir / &#39;train&#39; test_dir = ds_dir / &#39;test&#39; seed = 123 batch_size = 32 train_ds = text_dataset_from_directory( train_dir, label_mode=&#39;categorical&#39;, seed=seed, shuffle=True, batch_size=batch_size, validation_split=0.2, subset=&#39;training&#39; ) val_ds = text_dataset_from_directory( train_dir, label_mode=&#39;categorical&#39;, seed=seed, shuffle=True, batch_size=batch_size, validation_split=0.2, subset=&#39;validation&#39;) test_ds = text_dataset_from_directory( test_dir, label_mode=&#39;categorical&#39;, seed=seed, shuffle=True, batch_size=batch_size) . . Found 4500 files belonging to 50 classes. Using 3600 files for training. Found 4500 files belonging to 50 classes. Using 900 files for validation. Found 500 files belonging to 50 classes. . Inspect dataset . class_names = test_ds.class_names class_names = np.asarray(class_names) print(f&quot;nclasses: {len(class_names)}&quot;) print(f&#39;first 4 classes/users: {class_names[:4]}&#39;) for texts, labels in train_ds.take(1): print(&quot;Shape of texts&quot;, texts.shape) print(f&#39;Class of 2nd data point: {class_names[labels.numpy()[1].astype(bool)]}&#39;) . nclasses: 50 first 4 classes/users: [&#39;AaronPressman&#39; &#39;AlanCrosby&#39; &#39;AlexanderSmith&#39; &#39;BenjaminKangLim&#39;] Shape of texts (32,) Class of 2nd data point: [&#39;GrahamEarnshaw&#39;] . MAX_LEN_TRAIN = 0 MAX_LEN_TEST = 0 for file in test_dir.glob(&#39;*/*.txt&#39;): with file.open() as f: seq_len = 0 for line in f.readlines(): seq_len += len(line.split()) # print(seq_len) if MAX_LEN_TEST &lt; seq_len: MAX_LEN_TEST = seq_len for file in train_dir.glob(&#39;*/*.txt&#39;): with file.open() as f: seq_len = 0 for line in f.readlines(): seq_len += len(line.split()) # print(seq_len) if MAX_LEN_TRAIN &lt; seq_len: MAX_LEN_TRAIN = seq_len print(f&quot;length of largest article in train dataset: {MAX_LEN_TRAIN}&quot;) print(f&quot;length of largest article in test dataset: {MAX_LEN_TEST}&quot;) . . length of largest article in train dataset: 1498 length of largest article in test dataset: 1474 . for batch, label in iter(val_ds): index = np.argmax(label.numpy(), axis=1).astype(np.int) print(f&#39;Users of first batch: {class_names[index]}&#39;) break . . Users of first batch: [&#39;BradDorfman&#39; &#39;JaneMacartney&#39; &#39;RobinSidel&#39; &#39;JanLopatka&#39; &#39;GrahamEarnshaw&#39; &#39;SamuelPerry&#39; &#39;KouroshKarimkhany&#39; &#34;LynneO&#39;Donnell&#34; &#39;JaneMacartney&#39; &#39;FumikoFujisaki&#39; &#39;MarkBendeich&#39; &#39;LynnleyBrowning&#39; &#39;JanLopatka&#39; &#39;EdnaFernandes&#39; &#39;SimonCowell&#39; &#39;KirstinRidley&#39; &#39;MatthewBunce&#39; &#39;MichaelConnor&#39; &#39;KeithWeir&#39; &#39;HeatherScoffield&#39; &#39;MarcelMichelson&#39; &#39;PatriciaCommins&#39; &#39;MureDickie&#39; &#39;TanEeLyn&#39; &#39;MichaelConnor&#39; &#39;MureDickie&#39; &#39;MartinWolk&#39; &#39;TanEeLyn&#39; &#39;ScottHillis&#39; &#39;KirstinRidley&#39; &#39;ToddNissen&#39; &#39;MichaelConnor&#39;] . Text Vectorization . Text vectorization includes the following tasks using TextVectorization layer: . Standardization | Tokenization | Vectorization | Initial run of the vectorization layers . Make a text-only dataset (without labels), then call adapt | Do not call adapt on test dataset to prevent data-leak | train and save vocab to disk | Note: Use it only for the first time or if vocab is not saved . from utils import save_object ### Define vectorization layers VOCAB_SIZE = 34000 MAX_LEN = 1450 vectorize_layer = TextVectorization( max_tokens=VOCAB_SIZE, output_mode=&#39;int&#39;, output_sequence_length=MAX_LEN ) # Train the layers to learn a vocab train_text = train_ds.map(lambda text, lables: text) vectorize_layer.adapt(train_text) # Save the vocabulary to disk # Run this cell for the first time only vocab = vectorize_layer.get_vocabulary() vocab_path = Path(&#39;vocab/vocab_C50.pkl&#39;) save_object(vocab, vocab_path) vocab_len = len(vocab) print(f&quot;vocab size of vectorizer: {vocab_len}&quot;) . . pickle file vocab_C50.pkl already exists! object &lt;class &#39;list&#39;&gt; saved to file vocab_C50.pkl! vocab size of vectorizer: 34000 . Vectorization layers from saved vocab . Only run after first saving the vocabulary to the disk! . # # Load vocab # from utils import load_object # vocab_path = Path(&#39;vocab/vocab_C50.pkl&#39;) # vocab = load_object(vocab_path) # VOCAB_SIZE = 34000 # MAX_LEN = 1500 # vectorize_layer = TextVectorization( # max_tokens=VOCAB_SIZE, # output_mode=&#39;int&#39;, # output_sequence_length=MAX_LEN, # vocabulary=vocab # ) . . Configure dataset . This is the final step of the data-processing pipeline where the text is converted into vectors, and then to train the model faster, dataset is prefetched and cached before each epoch. Prefetch is especially efficient when training on a GPU as the CPU fetch and cache the dataset while GPU is training in parallel and after finishing current epoch GPU doesn&#39;t have to wait for CPU to load the data for next epoch. . def vectorize(text, label): text = tf.expand_dims(text, -1) return vectorize_layer(text), label AUTOTUNE = tf.data.AUTOTUNE def prepare(ds): return ds.cache().prefetch(buffer_size=AUTOTUNE) train_ds = train_ds.map(vectorize) val_ds = val_ds.map(vectorize) test_ds = test_ds.map(vectorize) # Configure the datasets for fast training train_ds = prepare(train_ds) val_ds = prepare(val_ds) test_ds = prepare(test_ds) . Modelling . Now comes the most interesting part! Below cell first creates an emb_index dictionary which maps words to a 100-Dimensional embedding vector and then emb_matrix is created which maps each word in C50 vocab to corresponding embedding. . ## Parse the weights from utils import load_object emb_dim = 100 glove_file = Path(f&#39;vocab/glove/glove.6B.{emb_dim}d.txt&#39;) emb_index = {} with glove_file.open(encoding=&#39;utf-8&#39;) as f: for line in f.readlines(): values = line.split() word = values[0] coef = values[1:] emb_index[word] = coef ##### Getting embedding weights ##### vocab = load_object(Path(&#39;vocab/vocab_C50.pkl&#39;)) emb_matrix = np.zeros((VOCAB_SIZE, emb_dim)) for index, word in enumerate(vocab): # get coef of word emb_vector = emb_index.get(word) if emb_vector is not None: emb_matrix[index] = emb_vector print(f&quot;Embedding Dimensionality: {emb_matrix.shape}&quot;) . loaded object from file vocab_C50.pkl Embedding Dimensionality: (34000, 100) . import keras.backend as K from keras.utils import plot_model from keras import regularizers K.clear_session() lstm_model = models.Sequential([ layers.Embedding(VOCAB_SIZE, emb_dim, input_shape=(MAX_LEN,)), layers.SpatialDropout1D(0.3), layers.Conv1D(256, 11, activation=&#39;relu&#39;), layers.MaxPooling1D(7), layers.Dropout(0.4), layers.BatchNormalization(), layers.Bidirectional(layers.LSTM(128, return_sequences=True)), layers.Dropout(0.3), layers.BatchNormalization(), layers.Conv1D(128, 3, activation=&#39;relu&#39;), layers.MaxPooling1D(3), layers.Dropout(0.3), layers.BatchNormalization(), layers.Conv1D(64, 3, activation=&#39;relu&#39;), layers.GlobalMaxPooling1D(), layers.Dropout(0.3), layers.BatchNormalization(), layers.Dense(128, activation=&#39;relu&#39;, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)), layers.BatchNormalization(), layers.Dense(50, activation=&#39;softmax&#39;) ]) . Detailed Architecture of the model . Load the emb_matrix as wieghts of the embedding layer of model and then set them as non-trainable. . lstm_model.layers[0].set_weights([emb_matrix]) lstm_model.layers[0].trainable = False plot_model(lstm_model, show_layer_names=False, show_shapes=True, to_file=&quot;models/base.png&quot;) . Training the model . K.clear_session() # optim = RMSprop(lr=1e-2) ################# Configure Callbacks ################# # Early Stopping es = cb.EarlyStopping( monitor=&#39;val_loss&#39;, min_delta=5e-4, patience=5, verbose=1, mode=&#39;auto&#39;, restore_best_weights=True ) # ReduceLROnPlateau reduce_lr = cb.ReduceLROnPlateau( monitor=&#39;val_loss&#39;, factor=0.4, patience=3, verbose=1, mode=&#39;auto&#39;, min_delta=5e-3, min_lr=1e-6 ) # Tensorboard tb = cb.TensorBoard( log_dir=&quot;./logs&quot;, write_graph=True, ) ################# Model Training ################# lstm_model.compile( loss=&#39;CategoricalCrossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;acc&#39;] ) lstm_history = lstm_model.fit( train_ds, validation_data=val_ds, epochs=100, callbacks=[es, reduce_lr, tb] ) lstm_model.save(&#39;models/base.h5&#39;) . Epoch 1/100 113/113 [==============================] - 35s 220ms/step - loss: 4.5336 - acc: 0.0188 - val_loss: 3.9713 - val_acc: 0.0200 Epoch 2/100 113/113 [==============================] - 13s 114ms/step - loss: 4.2080 - acc: 0.0312 - val_loss: 3.8599 - val_acc: 0.0278 Epoch 3/100 113/113 [==============================] - 13s 116ms/step - loss: 3.8997 - acc: 0.0403 - val_loss: 3.5842 - val_acc: 0.0544 Epoch 4/100 113/113 [==============================] - 13s 118ms/step - loss: 3.6826 - acc: 0.0454 - val_loss: 3.4762 - val_acc: 0.0467 Epoch 5/100 113/113 [==============================] - 13s 118ms/step - loss: 3.4679 - acc: 0.0585 - val_loss: 3.2102 - val_acc: 0.0900 Epoch 6/100 113/113 [==============================] - 13s 119ms/step - loss: 3.2866 - acc: 0.0862 - val_loss: 3.1259 - val_acc: 0.0844 Epoch 7/100 113/113 [==============================] - 13s 119ms/step - loss: 3.1113 - acc: 0.0972 - val_loss: 2.9419 - val_acc: 0.1533 Epoch 8/100 113/113 [==============================] - 17s 153ms/step - loss: 2.9159 - acc: 0.1459 - val_loss: 2.9488 - val_acc: 0.1456 Epoch 9/100 113/113 [==============================] - 17s 153ms/step - loss: 2.7023 - acc: 0.1737 - val_loss: 2.8818 - val_acc: 0.1544 Epoch 10/100 113/113 [==============================] - 17s 151ms/step - loss: 2.4595 - acc: 0.2387 - val_loss: 2.2731 - val_acc: 0.2889 Epoch 11/100 113/113 [==============================] - 17s 153ms/step - loss: 2.3077 - acc: 0.2764 - val_loss: 2.0811 - val_acc: 0.3033 Epoch 12/100 113/113 [==============================] - 17s 154ms/step - loss: 2.1691 - acc: 0.3154 - val_loss: 2.2417 - val_acc: 0.2622 Epoch 13/100 113/113 [==============================] - 18s 156ms/step - loss: 2.0237 - acc: 0.3493 - val_loss: 2.1130 - val_acc: 0.3178 Epoch 14/100 113/113 [==============================] - 17s 152ms/step - loss: 1.9448 - acc: 0.3544 - val_loss: 1.8156 - val_acc: 0.4133 Epoch 15/100 113/113 [==============================] - 17s 154ms/step - loss: 1.8491 - acc: 0.3828 - val_loss: 1.8381 - val_acc: 0.3789 Epoch 16/100 113/113 [==============================] - 17s 150ms/step - loss: 1.7508 - acc: 0.3922 - val_loss: 1.8668 - val_acc: 0.3789 Epoch 17/100 113/113 [==============================] - 18s 156ms/step - loss: 1.6681 - acc: 0.4339 - val_loss: 1.7776 - val_acc: 0.4022 Epoch 18/100 113/113 [==============================] - 17s 153ms/step - loss: 1.6276 - acc: 0.4328 - val_loss: 1.6306 - val_acc: 0.4411 Epoch 19/100 113/113 [==============================] - 17s 154ms/step - loss: 1.5808 - acc: 0.4466 - val_loss: 1.6069 - val_acc: 0.4522 Epoch 20/100 113/113 [==============================] - 17s 155ms/step - loss: 1.4749 - acc: 0.4696 - val_loss: 1.7228 - val_acc: 0.4089 Epoch 21/100 113/113 [==============================] - 17s 152ms/step - loss: 1.4379 - acc: 0.4849 - val_loss: 1.5438 - val_acc: 0.4567 Epoch 22/100 113/113 [==============================] - 17s 150ms/step - loss: 1.4126 - acc: 0.4781 - val_loss: 1.5012 - val_acc: 0.4667 Epoch 23/100 113/113 [==============================] - 17s 153ms/step - loss: 1.3508 - acc: 0.5022 - val_loss: 1.5094 - val_acc: 0.4467 Epoch 24/100 113/113 [==============================] - 17s 152ms/step - loss: 1.3220 - acc: 0.5124 - val_loss: 1.3901 - val_acc: 0.5067 Epoch 25/100 113/113 [==============================] - 17s 151ms/step - loss: 1.2896 - acc: 0.5279 - val_loss: 1.3944 - val_acc: 0.5056 Epoch 26/100 113/113 [==============================] - 17s 153ms/step - loss: 1.2639 - acc: 0.5314 - val_loss: 1.3389 - val_acc: 0.5244 Epoch 27/100 113/113 [==============================] - 17s 155ms/step - loss: 1.2449 - acc: 0.5533 - val_loss: 1.4641 - val_acc: 0.5044 Epoch 28/100 113/113 [==============================] - 17s 154ms/step - loss: 1.2253 - acc: 0.5574 - val_loss: 1.2490 - val_acc: 0.5756 Epoch 29/100 113/113 [==============================] - 17s 153ms/step - loss: 1.1860 - acc: 0.5534 - val_loss: 1.2377 - val_acc: 0.5744 Epoch 30/100 113/113 [==============================] - 17s 153ms/step - loss: 1.1334 - acc: 0.5763 - val_loss: 1.3690 - val_acc: 0.5100 Epoch 31/100 113/113 [==============================] - 17s 154ms/step - loss: 1.1058 - acc: 0.5856 - val_loss: 1.3175 - val_acc: 0.5678 Epoch 32/100 113/113 [==============================] - 17s 152ms/step - loss: 1.0955 - acc: 0.5932 - val_loss: 1.2472 - val_acc: 0.5667 Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805. Epoch 33/100 113/113 [==============================] - 17s 151ms/step - loss: 1.0554 - acc: 0.6074 - val_loss: 1.2565 - val_acc: 0.5433 Epoch 34/100 113/113 [==============================] - 17s 153ms/step - loss: 1.0444 - acc: 0.6218 - val_loss: 1.2164 - val_acc: 0.5711 Epoch 35/100 113/113 [==============================] - 17s 154ms/step - loss: 0.9743 - acc: 0.6205 - val_loss: 1.1523 - val_acc: 0.5956 Epoch 36/100 113/113 [==============================] - 17s 150ms/step - loss: 0.9554 - acc: 0.6343 - val_loss: 1.2389 - val_acc: 0.5656 Epoch 37/100 113/113 [==============================] - 18s 157ms/step - loss: 0.9462 - acc: 0.6412 - val_loss: 1.1904 - val_acc: 0.5767 Epoch 38/100 113/113 [==============================] - 18s 163ms/step - loss: 0.9341 - acc: 0.6527 - val_loss: 1.1958 - val_acc: 0.5922 Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222. Epoch 39/100 113/113 [==============================] - 18s 156ms/step - loss: 0.8854 - acc: 0.6792 - val_loss: 1.1309 - val_acc: 0.6078 Epoch 40/100 113/113 [==============================] - 17s 153ms/step - loss: 0.8495 - acc: 0.6747 - val_loss: 1.1413 - val_acc: 0.6022 Epoch 41/100 113/113 [==============================] - 18s 157ms/step - loss: 0.8854 - acc: 0.6635 - val_loss: 1.1481 - val_acc: 0.6044 Epoch 42/100 113/113 [==============================] - 17s 154ms/step - loss: 0.8528 - acc: 0.6802 - val_loss: 1.1488 - val_acc: 0.5967 Epoch 00042: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05. Epoch 43/100 113/113 [==============================] - 18s 155ms/step - loss: 0.8528 - acc: 0.6895 - val_loss: 1.1268 - val_acc: 0.6056 Epoch 44/100 113/113 [==============================] - 17s 154ms/step - loss: 0.8339 - acc: 0.6923 - val_loss: 1.1091 - val_acc: 0.6167 Epoch 45/100 113/113 [==============================] - 18s 156ms/step - loss: 0.8147 - acc: 0.6870 - val_loss: 1.1009 - val_acc: 0.6122 Epoch 46/100 113/113 [==============================] - 17s 153ms/step - loss: 0.8440 - acc: 0.6834 - val_loss: 1.1055 - val_acc: 0.6167 Epoch 47/100 113/113 [==============================] - 18s 157ms/step - loss: 0.8263 - acc: 0.6752 - val_loss: 1.1083 - val_acc: 0.6122 Epoch 48/100 113/113 [==============================] - 18s 158ms/step - loss: 0.7950 - acc: 0.7115 - val_loss: 1.1292 - val_acc: 0.6078 Epoch 00048: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05. Epoch 49/100 113/113 [==============================] - 18s 157ms/step - loss: 0.8392 - acc: 0.6807 - val_loss: 1.1152 - val_acc: 0.6133 Epoch 50/100 113/113 [==============================] - 17s 154ms/step - loss: 0.8248 - acc: 0.7110 - val_loss: 1.1076 - val_acc: 0.6167 Restoring model weights from the end of the best epoch. Epoch 00050: early stopping . print(&#39;Model evaluation on test dataset&#39;) lstm_model.evaluate(test_ds) # Plot training history plot_history( lstm_history, model_name=&quot;ConvnetBiLSTM&quot;, save_path=Path(&#39;plots/base.jpg&#39;) ) . Model evaluation on test dataset 16/16 [==============================] - 1s 58ms/step - loss: 1.4009 - acc: 0.5360 . Summary . The simple BiLSTM + Conv1D model performs fairly well considering that it was not tuned for performance and provides a good baseline to work with. A clear thing to note here is that, While performance of the model on validation dataset is quite good but it performs poorly on the test. Model doesn&#39;t generalize well on the holdout dataset which is our primary goal, in the text part I&#39;ll try to improve the accuracy to make it better than the baseline. .",
            "url": "https://www.ashishdev.codes/project/machine-learning/notebook/python/2021/04/27/ConvnetBiLSTM-C50.html",
            "relUrl": "/project/machine-learning/notebook/python/2021/04/27/ConvnetBiLSTM-C50.html",
            "date": " • Apr 27, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Digit Recognition Playground",
            "content": "Handwritten Digit Playground .",
            "url": "https://www.ashishdev.codes/app/2021/02/18/Digit-Recognition-Streamlit.html",
            "relUrl": "/app/2021/02/18/Digit-Recognition-Streamlit.html",
            "date": " • Feb 18, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Enable Fira Code and Ligatures in code cells ",
            "content": "This tutorial is beginner friendly so if you already know about fira code you might want to skip this part and move to Using Fira Code and Ligatures. . For those who are not familiar with fira code, it is one of the most popular fonts for coding, and the reason for that is - not only it’s an artistic font, it also provides Ligatures which are symbols for common programming multi-character combinations. . Let’s take a look at some examples . Character Combination: -&gt; . Ligature: . -&gt; . Character Combination: != . Ligature: . != . Character Combination: == . Ligature: . == . There’s no reason to discuss all the details about the font, head to wikipedia for more information or go to the FiraCode GitHub link if you want to use fira code anywhere else and for other examples of ligatures and use cases. . Using Fira Code and Ligatures . @import url(&#39;https://fonts.googleapis.com/css2?family=Fira+Code&amp;display=swap&#39;); .input_area pre, .input_area div, code { font-family: &#39;Fira Code&#39; !important; font-variant-ligatures: initial !important; } . Add these lines in /_sass/minima/custom-styles.scss at root of your repository and you are good to go. . . Fira Code font is only used in code cells of both notebook and markdown . Clear browser cache and wait for github actions to finish executing if the effects do not appear immediately Use this link to get my other font-styles and if you decide to create a new font-style.scss file as in my case, do not forget to add @import &quot;minima/font-style&quot;; in custom-styles.scss .",
            "url": "https://www.ashishdev.codes/markdown/tutorial/2020/12/30/Fira-Code.html",
            "relUrl": "/markdown/tutorial/2020/12/30/Fira-Code.html",
            "date": " • Dec 30, 2020"
        }
        
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "",
          "url": "https://www.ashishdev.codes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://www.ashishdev.codes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}