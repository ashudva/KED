{
  
    
  
    
  
    
        "post2": {
            "title": "Authorship Identification: Part-1 (The baseline)",
            "content": "Abstract . Authorship identification is the task of identifying the author of a given text from a set of suspects. The main concern of this task is to define an appropriate characterization of texts that captures the writing style of authors. In previous studies various stylometric models have been suggested for the aforementioned task e.g. BiLSTM, SVM, Logistic Regression, several other Deep Learning Models. State-of-the-art performance at authroship identification is achieved by BiLSTM but the authors used very small subset of the dataset for testing/holdout. . This article combines the stack of 1D-CNN with BiLSTM which gives the accuracy of 60% while using a fairly simple BiDirectional LSTM and CNN Architecture. . Model uses pretrained GloVe word Embeddings for text representation. GloVe uncased word embeddings were trined using Wikipedia 2014 + Gigaword 5 and it consists of 6B tokens, 400K vocab. Embeddings are available as 50d, 100d, 200d, &amp; 300d vectors. [Source] . Utilitiy functions . These are the functions that I&#39;ll be using to do redundant tasks in this part like: . Plotting train history | Saving figures | Saving and Loading pickle objects | take a look if interested! . import matplotlib.pyplot as plt import numpy as np import pickle from pathlib import Path def save_object(obj: object, file_path: Path) -&gt; None: &quot;&quot;&quot; Save a python object to the disk and creates the file if does not exists already. Args: file_path - Path object for pkl file location obj - object to be saved Returns: None &quot;&quot;&quot; if not file_path.exists(): file_path.touch() print(f&quot;pickle file {file_path.name} created successfully!&quot;) else: print(f&quot;pickle file {file_path.name} already exists!&quot;) with file_path.open(mode=&#39;wb&#39;) as file: pickle.dump(obj, file, protocol=pickle.HIGHEST_PROTOCOL) print(f&quot;object {type(obj)} saved to file {file_path.name}!&quot;) def load_object(file_path: Path) -&gt; object: &quot;&quot;&quot; Loads the pickle object file from the disk. Args: file_path - Path object for pkl file location Returns: object &quot;&quot;&quot; if file_path.exists(): with file_path.open(mode=&#39;rb&#39;) as file: print(f&quot;loaded object from file {file_path.name}&quot;) return pickle.load(file) else: raise FileNotFoundError def vectorize_sequence(sequences: np.ndarray, dimension: int = 10000): &quot;&quot;&quot; Convert sequences into one-hot encoded matrix of dimension [len(sequence), dimension] Args: sequences - ndarray of shape [samples, words] dimension = number of total words in vocab Return: vectorized sequence of shape [samples, one-hot-vecotor] &quot;&quot;&quot; # Create all-zero matrix results = np.zeros((len(sequences), dimension)) for (i, sequence) in enumerate(sequences): results[i, sequence] = 1. return results def plot_history( history: keras.callbacks.History, metric: str = &#39;acc&#39;, save_path: Path = None, model_name: str = None ) -&gt; None: &quot;&quot;&quot; Plots the history of training of a model during epochs Args: history: model history - training history of a model metric - Plots: 1. Training and Validation Loss 2. Training and Validation Accuracy &quot;&quot;&quot; f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5)) ax1.plot(history.epoch, history.history.get( &#39;loss&#39;), &quot;o&quot;, label=&#39;train loss&#39;) ax1.plot(history.epoch, history.history.get( &#39;val_loss&#39;), &#39;-&#39;, label=&#39;val loss&#39;) ax2.plot(history.epoch, history.history.get( metric), &#39;o&#39;, label=&#39;train acc&#39;) ax2.plot(history.epoch, history.history.get( f&quot;val_{metric}&quot;), &#39;-&#39;, label=&#39;val acc&#39;) ax1.set_xlabel(&quot;epoch&quot;) ax1.set_ylabel(&quot;loss&quot;) ax2.set_xlabel(&quot;epoch&quot;) ax2.set_ylabel(&quot;accuracy&quot;) ax1.set_title(&quot;Loss&quot;) ax2.set_title(&quot;Accuracy&quot;) f.suptitle(f&quot;Training History: {model_name}&quot;) ax1.legend() ax2.legend() if save_path is not None: f.savefig(save_path) . . Structure of notebook . Data Preprocessing a. Load dataset b. Text Vectorization c. Configure Dataset for faster training | Modelling a. Parse Glove Embeddings b. Define ConvnetBiLSTM model c. Load Embedding matrix | Training and Evaluation | Data Preprocessing . Dataset: UCI C50 Dataset(small subset of origin RCV1 dataset) [Source] C50 dataset is widely used for authorship identification. Dataset Specifications: . Catagories/Authors:50 &gt; Datapoints per class:50 &gt; Total Datapoints:2500 (train and test subsets each) . Load dataset . With 80-20 train and validation split. I&#39;ll use text_dataset_from_directory utility of keras library to load dataset which is faster than manually reading the text. . import keras import numpy as np import tensorflow as tf from pathlib import Path from utils import plot_history from keras import models, layers from keras.preprocessing import text_dataset_from_directory from keras.layers.experimental.preprocessing import TextVectorization ds_dir = Path(&#39;data/C50/&#39;) train_dir = ds_dir / &#39;train&#39; test_dir = ds_dir / &#39;test&#39; seed = 123 batch_size = 32 train_ds = text_dataset_from_directory( train_dir, label_mode=&#39;categorical&#39;, seed=seed, shuffle=True, batch_size=batch_size, validation_split=0.2, subset=&#39;training&#39; ) val_ds = text_dataset_from_directory( train_dir, label_mode=&#39;categorical&#39;, seed=seed, shuffle=True, batch_size=batch_size, validation_split=0.2, subset=&#39;validation&#39;) test_ds = text_dataset_from_directory( test_dir, label_mode=&#39;categorical&#39;, seed=seed, shuffle=True, batch_size=batch_size) . Inspect dataset . class_names = test_ds.class_names class_names = np.asarray(class_names) print(f&quot;nclasses: {len(class_names)}&quot;) print(f&#39;first 4 classes/users: {class_names[:4]}&#39;) for texts, labels in train_ds.take(1): print(&quot;Shape of texts&quot;, texts.shape) print(f&#39;Class of 2nd data point: {class_names[labels.numpy()[1].astype(bool)]}&#39;) . MAX_LEN_TRAIN = 0 MAX_LEN_TEST = 0 for file in test_dir.glob(&#39;*/*.txt&#39;): with file.open() as f: seq_len = 0 for line in f.readlines(): seq_len += len(line.split()) # print(seq_len) if MAX_LEN_TEST &lt; seq_len: MAX_LEN_TEST = seq_len for file in train_dir.glob(&#39;*/*.txt&#39;): with file.open() as f: seq_len = 0 for line in f.readlines(): seq_len += len(line.split()) # print(seq_len) if MAX_LEN_TRAIN &lt; seq_len: MAX_LEN_TRAIN = seq_len print(f&quot;length of largest article in train dataset: {MAX_LEN_TRAIN}&quot;) print(f&quot;length of largest article in test dataset: {MAX_LEN_TEST}&quot;) . for batch, label in iter(val_ds): index = np.argmax(label.numpy(), axis=1).astype(np.int) print(f&#39;Users of first batch: {class_names[index]}&#39;) break . Text Vectorization . Text vectorization includes the following tasks using TextVectorization layer: . Standardization | Tokenization | Vectorization | Initial run of the vectorization layers . Make a text-only dataset (without labels), then call adapt | Do not call adapt on test dataset to prevent data-leak | train and save vocab to disk | Note: Use it only for the first time or if vocab is not saved . from utils import save_object ### Define vectorization layers VOCAB_SIZE = 34000 MAX_LEN = 1500 vectorize_layer = TextVectorization( max_tokens=VOCAB_SIZE, output_mode=&#39;int&#39;, output_sequence_length=MAX_LEN ) # Train the layers to learn a vocab train_text = train_ds.map(lambda text, lables: text) vectorize_layer.adapt(train_text) # Save the vocabulary to disk # Run this cell for the first time only vocab = vectorize_layer.get_vocabulary() vocab_path = Path(&#39;vocab/vocab_C50.pkl&#39;) save_object(vocab, vocab_path) vocab_len = len(vocab) print(f&quot;vocab size of vectorizer: {vocab_len}&quot;) . Vectorization layers from saved vocab . Only run after first saving the vocabulary to the disk! . # from utils import load_object # vocab_path = Path(&#39;vocab/vocab_C50.pkl&#39;) # vocab = load_object(vocab_path) # VOCAB_SIZE = 34000 # MAX_LEN = 1500 # vectorize_layer = TextVectorization( # max_tokens=VOCAB_SIZE, # output_mode=&#39;int&#39;, # output_sequence_length=MAX_LEN, # vocabulary=vocab # ) . Configure dataset . def vectorize(text, label): text = tf.expand_dims(text, -1) return vectorize_layer(text), label AUTOTUNE = tf.data.AUTOTUNE def prepare(ds): return ds.cache().prefetch(buffer_size=AUTOTUNE) . train_ds = train_ds.map(vectorize) val_ds = val_ds.map(vectorize) test_ds = test_ds.map(vectorize) # Configure the datasets for fast training train_ds = prepare(train_ds) val_ds = prepare(val_ds) test_ds = prepare(test_ds) . for text, label in train_ds.take(1): print(text.shape) . Modelling . ## Parse the weights from utils import load_object emb_dim = 100 glove_file = Path(f&#39;vocab/glove/glove.6B.{emb_dim}d.txt&#39;) emb_index = {} with glove_file.open(encoding=&#39;utf-8&#39;) as f: for line in f.readlines(): values = line.split() word = values[0] coef = values[1:] emb_index[word] = coef ##### Getting embedding weights ##### vocab = load_object(Path(&#39;vocab/vocab_C50.pkl&#39;)) emb_matrix = np.zeros((VOCAB_SIZE, emb_dim)) for index, word in enumerate(vocab): # get coef of word emb_vector = emb_index.get(word) if emb_vector is not None: emb_matrix[index] = emb_vector print(emb_matrix.shape) . keras.backend.clear_session() lstm_model = models.Sequential([ layers.Embedding(VOCAB_SIZE, emb_dim, input_shape=(MAX_LEN,)), layers.Conv1D(256, 11, activation=&#39;relu&#39;), layers.MaxPooling1D(7), layers.Dropout(0.2), layers.Bidirectional(layers.LSTM(128, return_sequences=True)), layers.Conv1D(128, 3, activation=&#39;relu&#39;), layers.MaxPooling1D(3), layers.Dropout(0.4), layers.Conv1D(64, 3, activation=&#39;relu&#39;), layers.GlobalMaxPooling1D(), layers.Dense(128, activation=&#39;relu&#39;), layers.Dense(50, activation=&#39;softmax&#39;) ]) . lstm_model.layers[0].set_weights([emb_matrix]) lstm_model.layers[0].trainable = False lstm_model.summary() . from keras.optimizers import RMSprop optim = RMSprop(lr=1e-4) lstm_model.compile( loss=&#39;CategoricalCrossentropy&#39;, optimizer=optim, metrics=[&#39;acc&#39;] ) lstm_history = lstm_model.fit( train_ds, validation_data=val_ds, epochs=100 ) lstm_model.save(&#39;models/base.h5&#39;) . print(&#39;Model evaluation on test dataset&#39;) lstm_model.evaluate(test_ds) # Plot training history plot_history( lstm_history, model_name=&quot;ConvnetBiLSTM&quot; ) . Model evaluation on test dataset 16/16 [==============================] - 2s 98ms/step - loss: 2.5340 - acc: 0.5560 .",
            "url": "https://www.ashishdev.tech/project/machine-learning/notebook/python/2021/06/08/ConvnetBiLSTM-C50.html",
            "relUrl": "/project/machine-learning/notebook/python/2021/06/08/ConvnetBiLSTM-C50.html",
            "date": " • Jun 8, 2021"
        }
        
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m working on it! Thanks for your interest though. .",
          "url": "https://www.ashishdev.tech/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://www.ashishdev.tech/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}