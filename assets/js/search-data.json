{
  
    
  
    
        "post1": {
            "title": "Data Preprocessing",
            "content": "Load dataset . With 80-20 train and validation split . import keras import numpy as np import tensorflow as tf from pathlib import Path from utils import plot_history from keras import models, layers from keras.preprocessing import text_dataset_from_directory from keras.layers.experimental.preprocessing import TextVectorization ds_dir = Path(&#39;data/C50/&#39;) train_dir = ds_dir / &#39;train&#39; test_dir = ds_dir / &#39;test&#39; seed = 123 batch_size = 32 train_ds = text_dataset_from_directory( train_dir, label_mode=&#39;categorical&#39;, seed=seed, shuffle=True, batch_size=batch_size) val_ds = text_dataset_from_directory( test_dir, label_mode=&#39;categorical&#39;, seed=seed, shuffle=True, batch_size=batch_size, validation_split=0.2, subset=&#39;validation&#39;) test_ds = text_dataset_from_directory( test_dir, label_mode=&#39;categorical&#39;, seed=seed, shuffle=True, validation_split=0.2, subset=&#39;training&#39;, batch_size=128) . Found 2500 files belonging to 50 classes. Found 2500 files belonging to 50 classes. Using 500 files for validation. Found 2500 files belonging to 50 classes. Using 2000 files for training. . Inspect dataset . class_names = train_ds.class_names class_names = np.asarray(class_names) print(f&quot;nclasses: {len(class_names)}&quot;) print(f&#39;first 4 classes/users: {class_names[:4]}&#39;) for texts, labels in train_ds.take(1): print(&quot;Shape of texts&quot;, texts.shape) print(f&#39;Class of 2nd data point: {class_names[labels.numpy()[1].astype(bool)]}&#39;) . MAX_LEN = 0 for file in train_dir.glob(&#39;*/*.txt&#39;): with file.open() as f: seq_len = 0 for line in f.readlines(): seq_len += len(line.split()) # print(seq_len) if MAX_LEN &lt; seq_len: MAX_LEN = seq_len print(f&quot;length of largest article: {MAX_LEN}&quot;) . for batch, label in iter(val_ds): index = np.argmax(label.numpy(), axis=1).astype(np.int) print(f&#39;Users of first batch: {class_names[index]}&#39;) break . Text Vectorization . Text vectorization includes the following tasks using TextVectorization layer: . Standardization | Tokenization | Vectorization | Initial run of the vectorization layers . Make a text-only dataset (without labels), then call adapt | Do not call adapt on test dataset to prevent data-leak | train and save vocab to disk | Note: Use it only for the first time or if vocab is not saved . from utils import save_object ### Define vectorization layers VOCAB_SIZE = 34000 MAX_LEN = 1400 vectorize_layer = TextVectorization( max_tokens=VOCAB_SIZE, output_mode=&#39;int&#39;, output_sequence_length=MAX_LEN ) # Train the layers to learn a vocab train_text = train_ds.map(lambda text, lables: text) vectorize_layer.adapt(train_text) # Save the vocabulary to disk # Run this cell for the first time only vocab = vectorize_layer.get_vocabulary() vocab_path = Path(&#39;vocab/vocab_C50.pkl&#39;) save_object(vocab, vocab_path) vocab_len = len(vocab) print(f&quot;vocab size of vectorizer: {vocab_len}&quot;) . pickle file vocab_C50.pkl already exists! object &lt;class &#39;list&#39;&gt; saved to file vocab_C50.pkl! vocab size of vectorizer: 34000 . Vectorization layers from saved vocab . from utils import load_object vocab_path = Path(&#39;vocab/vocab_C50.pkl&#39;) vocab = load_object(vocab_path) VOCAB_SIZE = 34000 MAX_LEN = 1400 vectorize_layer = TextVectorization( max_tokens=VOCAB_SIZE, output_mode=&#39;int&#39;, output_sequence_length=MAX_LEN, vocabulary=vocab ) . len(vocab) . Process dataset through layers . def vectorize(text, label): text = tf.expand_dims(text, -1) return vectorize_layer(text), label AUTOTUNE = tf.data.AUTOTUNE def prepare(ds): return ds.cache().prefetch(buffer_size=AUTOTUNE) . train_ds = train_ds.map(vectorize) val_ds = val_ds.map(vectorize) test_ds = test_ds.map(vectorize) # Configure the datasets for fast training train_ds = prepare(train_ds) val_ds = prepare(val_ds) test_ds = prepare(test_ds) . for text, label in train_ds.take(1): print(text.shape) . (32, 1400) . Modelling and Experimentation . Base model . ## Parse the weights from utils import load_object emb_dim = 100 glove_file = Path(f&#39;vocab/glove/glove.6B.{emb_dim}d.txt&#39;) emb_index = {} with glove_file.open(encoding=&#39;utf-8&#39;) as f: for line in f.readlines(): values = line.split() word = values[0] coef = values[1:] emb_index[word] = coef ##### Getting embedding weights ##### vocab = load_object(Path(&#39;vocab/vocab_C50.pkl&#39;)) emb_matrix = np.zeros((VOCAB_SIZE, emb_dim)) for index, word in enumerate(vocab): # get coef of word emb_vector = emb_index.get(word) if emb_vector is not None: emb_matrix[index] = emb_vector print(emb_matrix.shape) . loaded object from file vocab_C50.pkl (34000, 100) . keras.backend.clear_session() lstm_model = models.Sequential([ layers.Embedding(VOCAB_SIZE, emb_dim, input_shape=(MAX_LEN,)), layers.Conv1D(256, 11, activation=&#39;relu&#39;), layers.MaxPooling1D(7), layers.Dropout(0.4), layers.Bidirectional(layers.LSTM(128, return_sequences=True)), layers.Conv1D(128, 3, activation=&#39;relu&#39;), layers.MaxPooling1D(3), layers.Dropout(0.2), layers.Conv1D(64, 3, activation=&#39;relu&#39;), layers.GlobalMaxPooling1D(), layers.Dense(128, activation=&#39;relu&#39;), layers.Dense(50, activation=&#39;softmax&#39;) ]) lstm_model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, 1400, 100) 3400000 _________________________________________________________________ conv1d (Conv1D) (None, 1390, 256) 281856 _________________________________________________________________ max_pooling1d (MaxPooling1D) (None, 198, 256) 0 _________________________________________________________________ dropout (Dropout) (None, 198, 256) 0 _________________________________________________________________ bidirectional (Bidirectional (None, 198, 256) 394240 _________________________________________________________________ conv1d_1 (Conv1D) (None, 196, 128) 98432 _________________________________________________________________ max_pooling1d_1 (MaxPooling1 (None, 65, 128) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 65, 128) 0 _________________________________________________________________ conv1d_2 (Conv1D) (None, 63, 64) 24640 _________________________________________________________________ global_max_pooling1d (Global (None, 64) 0 _________________________________________________________________ dense (Dense) (None, 128) 8320 _________________________________________________________________ dense_1 (Dense) (None, 50) 6450 ================================================================= Total params: 4,213,938 Trainable params: 4,213,938 Non-trainable params: 0 _________________________________________________________________ . lstm_model.layers[0].set_weights([emb_matrix]) lstm_model.layers[0].trainable = False lstm_model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, 1400, 100) 3400000 _________________________________________________________________ conv1d (Conv1D) (None, 1390, 256) 281856 _________________________________________________________________ max_pooling1d (MaxPooling1D) (None, 198, 256) 0 _________________________________________________________________ dropout (Dropout) (None, 198, 256) 0 _________________________________________________________________ bidirectional (Bidirectional (None, 198, 256) 394240 _________________________________________________________________ conv1d_1 (Conv1D) (None, 196, 128) 98432 _________________________________________________________________ max_pooling1d_1 (MaxPooling1 (None, 65, 128) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 65, 128) 0 _________________________________________________________________ conv1d_2 (Conv1D) (None, 63, 64) 24640 _________________________________________________________________ global_max_pooling1d (Global (None, 64) 0 _________________________________________________________________ dense (Dense) (None, 128) 8320 _________________________________________________________________ dense_1 (Dense) (None, 50) 6450 ================================================================= Total params: 4,213,938 Trainable params: 813,938 Non-trainable params: 3,400,000 _________________________________________________________________ . from keras.optimizers import RMSprop optim = RMSprop(lr=1e-4) lstm_model.compile( loss=&#39;CategoricalCrossentropy&#39;, optimizer=optim, metrics=[&#39;acc&#39;] ) lstm_history = lstm_model.fit( train_ds, validation_data=val_ds, epochs=100 ) lstm_model.save(&#39;models/base.h5&#39;) . print(&#39;Model evaluation on test dataset&#39;) lstm_model.evaluate(test_ds) # Plot training history from utils import plot_history plot_history( lstm_history, model_name=&quot;ConvnetBiLSTM&quot; ) . Model evaluation on test dataset 16/16 [==============================] - 2s 129ms/step - loss: 3.1715 - acc: 0.5080 .",
            "url": "https://www.ashishdev.tech/2021/05/26/ConvnetBiLSTM-C50.html",
            "relUrl": "/2021/05/26/ConvnetBiLSTM-C50.html",
            "date": " • May 26, 2021"
        }
        
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m working on it! Thanks for your interest though. .",
          "url": "https://www.ashishdev.tech/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://www.ashishdev.tech/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}