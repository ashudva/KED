{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Authorship Identification: Part-2 (DistilBERT Transformer)\"\r\n",
    "> \"Using transfer-learning to fine-tune pretrained DistilBERT transformer for authorship identification. In a nutshell, DistilBERT is a small version of BERT which is \"smaller, faster, cheaper, and lighter\". It has 40% less parameters original BERT, runs 60% faster and preserve over 95% of BERTâ€™s performances (measured on the GLUE language understanding benchmark).\"\r\n",
    "\r\n",
    "- toc: false\r\n",
    "- sticky_rank: 2\r\n",
    "- branch: master\r\n",
    "- badges: true\r\n",
    "- comments: true\r\n",
    "- categories: [project, machine-learning, notebook, python, transformers, huggingface]\r\n",
    "- image: images/vignette/huggingface.jpg\r\n",
    "- hide: false\r\n",
    "- search_exclude: false\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\r\n",
    "**This is a follow-up post on the authorship identification project.**<br/>\r\n",
    "I regard the past few years as the inception of the era of Transformers which started with the popular Research Paper \"Attention is all you need\" by \"somebody\" in 2020. Several transformer architectures have shown up since then. Some of the famous ones are - GPT, GPT2, and the latest GPT3 which has outperformed many previous state-of-the-art models at several tasks in NLP, BERT (by Google) is also one of the most popular transformers out there.<br/>\r\n",
    "Transformers are very large models with multi-billions of parameters. Pretrained transformers have shown tremendous capability when used with a downstream task head in Transfer Learning similar to the CNNs in Computer Vision.<br/>\r\n",
    "In this part, I'll use fine-tuned DistilBERT transformer which is a smaller version of the original BERT for the downstream classification task.<br/>\r\n",
    "I'll use the `transformers` library from Huggingface which consists of numerous state-of-the-art transformers and supports several downstream tasks out of the box. In short, I consider Huggingface a great starting point for a person engrossed in NLP and it offers tons of great functionalities.<br/>\r\n",
    "I'll provide links to resources for you to learn more about these technologies. \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2500 files belonging to 50 classes.\n",
      "Using 2000 files for training.\n",
      "Found 2500 files belonging to 50 classes.\n",
      "Using 500 files for validation.\n",
      "Found 2500 files belonging to 50 classes.\n"
     ]
    }
   ],
   "source": [
    "# Imports\r\n",
    "import keras\r\n",
    "import tensorflow as tf\r\n",
    "import numpy as np\r\n",
    "from pathlib import Path\r\n",
    "from utils import plot_history\r\n",
    "from keras.preprocessing import text_dataset_from_directory\r\n",
    "\r\n",
    "ds_dir = Path('data/C50/')\r\n",
    "train_dir = ds_dir / 'train'\r\n",
    "test_dir = ds_dir / 'test'\r\n",
    "seed = 1000\r\n",
    "batch_size = 16\r\n",
    "\r\n",
    "\r\n",
    "train_ds = text_dataset_from_directory(train_dir,\r\n",
    "                                     label_mode='int',\r\n",
    "                                     seed=seed,\r\n",
    "                                     shuffle=True,\r\n",
    "                                     validation_split=0.2,\r\n",
    "                                     subset='training')\r\n",
    "\r\n",
    "val_ds = text_dataset_from_directory(train_dir,\r\n",
    "                                      label_mode='int',\r\n",
    "                                      seed=seed,\r\n",
    "                                      shuffle=True,\r\n",
    "                                      validation_split=0.2,\r\n",
    "                                     subset='validation')\r\n",
    "\r\n",
    "test_ds = text_dataset_from_directory(test_dir,\r\n",
    "                                       label_mode='int',\r\n",
    "                                       seed=seed,\r\n",
    "                                       shuffle=True,\r\n",
    "                                       batch_size=batch_size)\r\n",
    "\r\n",
    "class_names = train_ds.class_names\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare and Configure the datasets\r\n",
    "from utils import get_text, prepare_batched\r\n",
    "from transformers import DistilBertTokenizerFast\r\n",
    "\r\n",
    "AUTOTUNE = tf.data.AUTOTUNE\r\n",
    "\r\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\r\n",
    "\r\n",
    "batch_size=2\r\n",
    "\r\n",
    "train_ds = prepare_batched(train_ds, tokenizer, batch_size=batch_size)\r\n",
    "val_ds = prepare_batched(val_ds, tokenizer, batch_size=batch_size)\r\n",
    "test_ds = prepare_batched(test_ds, tokenizer, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning the model\r\n",
    "keras.backend.clear_session()\r\n",
    "from transformers import TFAutoModelForSequenceClassification\r\n",
    "\r\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=50)\r\n",
    "\r\n",
    "model.compile(\r\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=5e-5),\r\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy()\r\n",
    ")\r\n",
    "\r\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=20)\r\n",
    "\r\n",
    "plot_history(history, 'sparse_categorical_accuracy')\r\n",
    "model.save(\"DistilBERT_finetuned.h5\")\r\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {},
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}